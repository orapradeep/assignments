{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library for numerical methods. We will use this extensively in this course!\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bisection Algorithm\n",
    "\n",
    "Suppose we want to find $x$ that solves the equation\n",
    "$$ x^2 = 3 $$\n",
    "\n",
    "Further suppose that we don't have a calculator that can compute square roots. We do have a calculator that can do basic math operations (+,-,Ã—). \n",
    "\n",
    "Do these restrictions seem arbitrary? It turns out, these basic operations can be computed using analog circuits. This means that our computer *hardware* is capable of computing sums, differences, and products. Other operations, such as division or square roots, are actually computed using algorithms which combine these elementary operations. In other words, any time you use the \"square root\" button on your calculator, an algorithm such as the one that follows is executed under-the-hood.\n",
    "\n",
    "In this exercise, we study a basic rootfinding algorithm. The goal is to begin to appreciate how algorithms are used to solve math problems. In general, we cannot solve our problems exactly. Instead, a properly designed algorithm will provide us an estimate which is arbitrary close to the exact solution. In other words, we cannot compute $\\sqrt{3}$, which is an irrational number with infintitely many digits, but we can arrive at an approximation that is within $10^{-12}$ of the correct answer (i.e., we can find an answer correct to 12 digits after the decimal point).\n",
    "\n",
    "Let's begin by reframing the problem as a rootfinding problem. Set\n",
    "$$f(x) = x^2 - 3$$\n",
    "\n",
    "Note that if we can find $x$ such that $f(x)=0$, we have found $x$ such that $x^2=3$. \n",
    "\n",
    "Let's plot the function $f$ and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*x-3\n",
    "\n",
    "x = np.linspace(0,4,100)  # 100 evenly spaced points between 0 and 4\n",
    "plt.plot(x,f(x))\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the following:\n",
    "$$ f(1) = -2 $$\n",
    "$$ f(2) = 1 $$\n",
    "\n",
    "We know that the solution we are looking for is between $x=1$ and $x=2$. Let's use these two points as our initial *bracket*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial bracket\n",
    "a  = 1\n",
    "b  = 2\n",
    "\n",
    "# Current approximation of the solution:\n",
    "c = (a+b)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use these convenience functions to get printouts of our current approximations\n",
    "\n",
    "def print_header():\n",
    "    # This is string formatting, which we've already seen. \n",
    "    # The numbers in the format code control the spacing. This gives us a pretty(ish) table.\n",
    "    print \"%17s %20s %16s %9s %9s %9s %13s\"%(\"lower bound (a)\",\"current approx (c)\",\"upper bound (b)\", \"f(a)\",\"f(c)\",\"f(b)\",\"uncertainty\")\n",
    "\n",
    "def print_approximation(a,b,c):\n",
    "    # Note that the numbers are the same though the datatypes differ\n",
    "    print \"%17f %20f %16f %9.6f %9.6f %9.6f %13e\"%(a,c,b,f(a),f(c),f(b),(b-a)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_header()\n",
    "print_approximation(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, an approximation of the answer to our problem is the simple mean of our bracket. That is, we know that $x = 1.5 \\pm 0.5$. To reduce the uncertainty we must reduce the size of our bracket.\n",
    "\n",
    "We can see from the chart above that the solution is between $c$ and $b$. For our next approximation, we will select $c$ to be the new lower bound, discard $a$, and evaluate a new approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = c # New lower bound\n",
    "c = (a+b)*0.5 # New approximation\n",
    "print_header()\n",
    "print_approximation(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that the $x=1.75 \\pm 0.25$. We've reduced our uncertainty by half... not bad for one iteration. The power of an algorithm, though, is that we can reduce the uncertainty to an arbitrarily small amount by simply repeating the procedure until we are satisfied. This is **iteration**, the repeated application of a procedure which leads to increasingly-accurate approximations.\n",
    "\n",
    "Let's suppose we want to reduce our uncertainty to $10^{-5}$. We can use the following procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial bracket and approximation\n",
    "a  = 1\n",
    "b  = 2\n",
    "c = (a+b)*0.5\n",
    "\n",
    "tol = 1e-5\n",
    "uncertainty = (b-a)*0.5\n",
    "\n",
    "print_header()\n",
    "\n",
    "while(uncertainty>tol):\n",
    "    c = (a+b)*0.5  # Current approximation\n",
    "    print_approximation(a,b,c)\n",
    "    \n",
    "    if f(a)*f(c)<0:    # f(a) and f(c) have opposite signs. The solution is between them. Discard b to shrink our bracket\n",
    "        b = c\n",
    "    elif f(b)*f(c)<0:  # Solution is between b and c\n",
    "        a = c\n",
    "    elif f(c)==0:\n",
    "        print \"found exact solution: c=\",c\n",
    "        break\n",
    "    else:\n",
    "        print \"Something went pathologically wrong!\"\n",
    "        break\n",
    "            \n",
    "    # Update uncertainty (very important! )\n",
    "    uncertainty = (b-a)*0.5\n",
    "\n",
    "print\n",
    "print \"Algorithm complete. Final result:\"\n",
    "print_header()\n",
    "print_approximation(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compare our answer to Python's\n",
    "err = abs(c-3.**0.5)\n",
    "print \"Error of final approximation: \", err\n",
    "print \"Final uncertainty: \", (b-a)*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find a solution to the problem:\n",
    "$$ \\sin(x) = \\ln(x)$$\n",
    "\n",
    "*Hint:* You can use `np.sin()` and `np.log()` functions for $\\sin()$ and $\\ln()$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
